import pandas as pd

data=pd.read_csv('dataset_final.csv')
odds=pd.read_csv('odds.csv')
data.head()
odds.head()


# constuct the test set

data.loc[:, 'season']
df=data
#test=df.where(df['season']==2017)
df17_logic=df['season']==2017
df17_logic.head()
df17_logic
df17=df.loc[df17_logic,:]
df17




# construct training set
df_train_logic=df['season']!=2017
df_train=df.loc[df_train_logic,:]
df_train

result=pd.merge(data, odds, on='eventId')
result.head()

c=0
goodbet1=0
goodbet2=0
goodbetX=0


badbet=0
for line in result.result:
    if line=='1' and  (result.PR_1[c] >result.PR_2[c] and result.PR_1[c]>result.PR_X[c]):
        goodbet1+=1
    elif line=='2' and  (result.PR_2[c] >result.PR_1[c] and result.PR_2[c]>result.PR_X[c]):
        goodbet2+=1
    elif line=='X' and  (result.PR_X[c] >result.PR_1[c] and result.PR_X[c]>result.PR_2[c]):
        goodbetX+=1
    else:
        badbet+=1
    c+=1
    
goodbet=goodbet1+goodbet2+goodbetX
print('Good bets: ', goodbet1, goodbet2, goodbetX, goodbet,  'Bad bets: ' , badbet)

unique_teams= data.home_teamId.unique()
print(unique_teams)


# Number of victories per team

victories={}
for team in unique_teams:
    res1=data[data.home_teamId==team]
    count=0
    for i in res1.result:
        if i=='1':
            count+=1
    res2=data[data.away_teamId==team]
    for i in res2.result:
        if i=='2':
            count+=1            
    victories[team]= count
print(victories)

df_clean=df_train[(df_train.season!=2014) | (df_train.week!=1)].dropna()
df_clean=df_clean.drop(['timestamp'], axis=1)
df_clean=df_clean.drop(['eventId'], axis=1)
df_clean=df_clean.drop(['stadio_id'], axis=1)
df_clean.describe()
df_clean_X=df_clean.drop(['result'], axis=1)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

pca = PCA()
pc = pca.fit_transform(df_clean_X)
pc_df = pd.DataFrame(pc)

print(pca.explained_variance_ratio_.cumsum())



#sns.pairplot(pc_df)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
# Build the pipeline
pipe = Pipeline([('scaler', StandardScaler()),
        		 ('reducer', PCA(n_components=8))])

# Fit it to the dataset and extract the component vectors
pipe.fit(df_clean_X)
vectors = pipe.steps[1][1].components_.round(2)

# Print feature effects
print('PC 1 effects = ' + str(dict(zip(df_clean_X.columns, vectors[0]))))
print('PC 2 effects = ' + str(dict(zip(df_clean_X.columns, vectors[1]))))
print('PC 3 effects = ' + str(dict(zip(df_clean_X.columns, vectors[2]))))
print('PC 4 effects = ' + str(dict(zip(df_clean_X.columns, vectors[3]))))


